<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="Jose Parreno Garcia" />

<meta name="date" content="2017-09-14" />

<title>Point Pattern Analysis</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/spacelab.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script src="site_libs/htmlwidgets-0.9/htmlwidgets.js"></script>
<script src="site_libs/plotly-binding-4.7.1/plotly.js"></script>
<script src="site_libs/typedarray-0.1/typedarray.min.js"></script>
<link href="site_libs/crosstalk-1.0.0/css/crosstalk.css" rel="stylesheet" />
<script src="site_libs/crosstalk-1.0.0/js/crosstalk.min.js"></script>
<link href="site_libs/plotlyjs-1.29.2/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="site_libs/plotlyjs-1.29.2/plotly-latest.min.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; background-color: #f8f8f8; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
pre, code { background-color: #f8f8f8; }
code > span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code > span.dt { color: #204a87; } /* DataType */
code > span.dv { color: #0000cf; } /* DecVal */
code > span.bn { color: #0000cf; } /* BaseN */
code > span.fl { color: #0000cf; } /* Float */
code > span.ch { color: #4e9a06; } /* Char */
code > span.st { color: #4e9a06; } /* String */
code > span.co { color: #8f5902; font-style: italic; } /* Comment */
code > span.ot { color: #8f5902; } /* Other */
code > span.al { color: #ef2929; } /* Alert */
code > span.fu { color: #000000; } /* Function */
code > span.er { color: #a40000; font-weight: bold; } /* Error */
code > span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #000000; } /* Constant */
code > span.sc { color: #000000; } /* SpecialChar */
code > span.vs { color: #4e9a06; } /* VerbatimString */
code > span.ss { color: #4e9a06; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #000000; } /* Variable */
code > span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code > span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code > span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code > span.ex { } /* Extension */
code > span.at { color: #c4a000; } /* Attribute */
code > span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code > span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 52px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 57px;
  margin-top: -57px;
}

.section h2 {
  padding-top: 57px;
  margin-top: -57px;
}
.section h3 {
  padding-top: 57px;
  margin-top: -57px;
}
.section h4 {
  padding-top: 57px;
  margin-top: -57px;
}
.section h5 {
  padding-top: 57px;
  margin-top: -57px;
}
.section h6 {
  padding-top: 57px;
  margin-top: -57px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->






<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">My Website</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Spatial Point Pattern Analysis - Introduction</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Point Pattern Analysis</h1>
<h4 class="author"><em>Jose Parreno Garcia</em></h4>
<h4 class="date"><em>14 September 2017</em></h4>

</div>

<div id="TOC">
<ul>
<li><a href="#setting-the-problem"><span class="toc-section-number">1</span> Setting the problem</a><ul>
<li><a href="#examples-of-point-patterns"><span class="toc-section-number">1.1</span> Examples of point patterns</a></li>
</ul></li>
<li><a href="#models-of-spatial-randomness"><span class="toc-section-number">2</span> Models of Spatial Randomness</a><ul>
<li><a href="#spatial-laplace-principle"><span class="toc-section-number">2.1</span> Spatial Laplace Principle</a></li>
<li><a href="#complete-spatial-randomness-csr"><span class="toc-section-number">2.2</span> Complete Spatial Randomness (CSR)</a><ul>
<li><a href="#for-small-regions"><span class="toc-section-number">2.2.1</span> For small regions</a></li>
<li><a href="#for-large-regions"><span class="toc-section-number">2.2.2</span> For large regions</a></li>
</ul></li>
</ul></li>
<li><a href="#testing-csr---basics"><span class="toc-section-number">3</span> Testing CSR - Basics</a><ul>
<li><a href="#quadrat-method"><span class="toc-section-number">3.1</span> Quadrat method</a><ul>
<li><a href="#the-pearson-chi2-goodness-of-fit-test"><span class="toc-section-number">3.1.1</span> The Pearson <span class="math inline">\(\chi^{2}\)</span> goodness-of-fit test:</a></li>
<li><a href="#creating-pearson-chi2-goodness-of-fit-test-function"><span class="toc-section-number">3.1.2</span> Creating Pearson <span class="math inline">\(\chi^{2}\)</span> goodness-of-fit test function</a></li>
<li><a href="#testing-with-r-functions"><span class="toc-section-number">3.1.3</span> Testing with R functions</a></li>
<li><a href="#problems-with-quadrat-methods."><span class="toc-section-number">3.1.4</span> Problems with quadrat methods.</a></li>
</ul></li>
<li><a href="#nearest-neighbour-method"><span class="toc-section-number">3.2</span> Nearest Neighbour Method</a><ul>
<li><a href="#nn-method-under-csr"><span class="toc-section-number">3.2.1</span> NN-method under CSR</a></li>
<li><a href="#example"><span class="toc-section-number">3.2.2</span> Example</a></li>
<li><a href="#problems-with-nn-methods"><span class="toc-section-number">3.2.3</span> Problems with NN-methods</a></li>
</ul></li>
<li><a href="#k-functions"><span class="toc-section-number">3.3</span> K functions</a><ul>
<li><a href="#building-the-formula"><span class="toc-section-number">3.3.1</span> Building the formula</a></li>
<li><a href="#k-and-l-function-under-csr"><span class="toc-section-number">3.3.2</span> K and L function under CSR</a></li>
<li><a href="#testing-the-bodmin-tor-example"><span class="toc-section-number">3.3.3</span> Testing the Bodmin Tor example</a></li>
</ul></li>
</ul></li>
</ul>
</div>

<style>
body {
text-align: justify}
</style>
<p><br></p>
<center>
<strong><em>This page is based on the superb materials that Tony E. Smith created for his ESE 502 subject on spatial statistics.</em></strong>
</center>
<p><br></p>
<center>
<a href="https://www.seas.upenn.edu/~ese502" class="uri">https://www.seas.upenn.edu/~ese502</a>
</center>
<p><br></p>
<p>I follow the same structure and pretty much all the examples that Tony E. Smith uses the material for lectures. The intention is not produce a scientific document, but a place to have as a reference to understand spatial point pattern analysis easily with some support R code (i.e. do not expect derivation of formulas, or complex writing.)</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># ----------------------------------------------------------------------</span>
<span class="co"># Loading libraries</span>
<span class="co"># ----------------------------------------------------------------------</span>
<span class="co"># Dataset libraries</span>
<span class="kw">library</span>(boot)

<span class="co"># Plotting libraries</span>
<span class="kw">library</span>(ggplot2)
<span class="kw">library</span>(plotly)
<span class="kw">library</span>(gridExtra)
<span class="kw">library</span>(grid)
<span class="kw">library</span>(png)

<span class="co"># Spatial libraries</span>
<span class="kw">library</span>(sp)
<span class="kw">library</span>(spatstat)
<span class="kw">library</span>(spatial)
<span class="kw">library</span>(splancs)

<span class="co"># Statistics library</span>
<span class="kw">library</span>(stats)

<span class="co"># Other</span>
<span class="kw">library</span>(knitr)</code></pre></div>
<p><br></p>
<div id="setting-the-problem" class="section level1">
<h1><span class="header-section-number">1</span> Setting the problem</h1>
<p><br></p>
<div id="examples-of-point-patterns" class="section level2">
<h2><span class="header-section-number">1.1</span> Examples of point patterns</h2>
<p><br></p>
<p>Let’s look at 2 differentiated types of point patterns.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Redwood window</span>
redwood_rr =<span class="st"> </span><span class="kw">ppp</span>(redwood$x, redwood$y, <span class="dt">window =</span> <span class="kw">ripras</span>(redwood))

<span class="co"># Cells window</span>
cells_rr =<span class="st"> </span><span class="kw">ppp</span>(cells$x, cells$y, <span class="dt">window =</span> <span class="kw">ripras</span>(cells))

<span class="co"># Subplots</span>
<span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))
<span class="kw">plot</span>(redwood_rr, <span class="dt">cols=</span><span class="st">&#39;brown&#39;</span>, <span class="dt">main=</span><span class="st">&#39;Redwood Seedling&#39;</span>)  <span class="co"># pch is symbol code, cex is plot shrinkage</span>
<span class="kw">plot</span>(cells_rr, <span class="dt">cols=</span><span class="st">&#39;green&#39;</span>, <span class="dt">main=</span><span class="st">&#39;Cells&#39;</span>)  <span class="co"># pch is symbol code, cex is plot shrinkage factor</span></code></pre></div>
<p><img src="index_files/figure-html/unnamed-chunk-28-1.png" /><!-- --></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">xlabs =<span class="st"> </span><span class="kw">list</span>(<span class="dt">title =</span> <span class="st">&quot;&quot;</span>, <span class="dt">linecolor =</span> <span class="kw">toRGB</span>(<span class="st">&quot;lightgrey&quot;</span>), <span class="dt">zerolinecolor =</span> <span class="kw">toRGB</span>(<span class="st">&quot;white&quot;</span>))
ylabs =<span class="st"> </span><span class="kw">list</span>(<span class="dt">title =</span> <span class="st">&quot;&quot;</span>, <span class="dt">linecolor =</span> <span class="kw">toRGB</span>(<span class="st">&quot;lightgrey&quot;</span>), <span class="dt">zerolinecolor =</span> <span class="kw">toRGB</span>(<span class="st">&quot;white&quot;</span>))

<span class="co"># ---------------------------------------------------------------------------------------------------------</span>
<span class="co"># Redwood seedlings dataset</span>
<span class="co"># ---------------------------------------------------------------------------------------------------------</span>
redwood_df =<span class="st"> </span><span class="kw">data.frame</span>(<span class="kw">matrix</span>(<span class="kw">vector</span>(), redwood$n, <span class="dv">2</span>
                               , <span class="dt">dimnames =</span> <span class="kw">list</span>(<span class="kw">c</span>(), <span class="kw">c</span>(<span class="st">&quot;x&quot;</span>,<span class="st">&quot;y&quot;</span>))), <span class="dt">stringsAsFactors =</span> F)
redwood_df$x =<span class="st"> </span>redwood$x
redwood_df$y =<span class="st"> </span>redwood$y

a1 =<span class="st"> </span><span class="kw">list</span>(<span class="dt">text =</span> <span class="st">&quot;Fig 1.1. Redwood seedlings&quot;</span>, <span class="dt">showarrow =</span> <span class="ot">FALSE</span>, <span class="dt">xref =</span> <span class="st">&quot;paper&quot;</span>, <span class="dt">yref =</span> <span class="st">&quot;paper&quot;</span>, <span class="dt">yanchor =</span> <span class="st">&quot;bottom&quot;</span>,
  <span class="dt">xanchor =</span> <span class="st">&quot;center&quot;</span>, <span class="dt">align =</span> <span class="st">&quot;center&quot;</span>, <span class="dt">x =</span> <span class="fl">0.5</span>, <span class="dt">y =</span> <span class="dv">1</span>)
p1 =<span class="st"> </span><span class="kw">plot_ly</span>(redwood_df, <span class="dt">x =</span> ~x, <span class="dt">y =</span> ~y) %&gt;%<span class="st"> </span><span class="kw">layout</span>(<span class="dt">annotations =</span> a1, <span class="dt">xaxis =</span> xlabs, <span class="dt">yaxis =</span> ylabs)

<span class="co"># Number points</span>
n =<span class="st"> </span>redwood$n

<span class="co"># Redwood window</span>
redwood_rr =<span class="st"> </span><span class="kw">ppp</span>(redwood$x, redwood$y, <span class="dt">window =</span> <span class="kw">ripras</span>(redwood))

<span class="co"># ---------------------------------------------------------------------------------------------------------</span>
<span class="co"># Biological cell dataset</span>
<span class="co"># ---------------------------------------------------------------------------------------------------------</span>
cells_df =<span class="st"> </span><span class="kw">data.frame</span>(<span class="kw">matrix</span>(<span class="kw">vector</span>(), cells$n, <span class="dv">2</span>
                               , <span class="dt">dimnames =</span> <span class="kw">list</span>(<span class="kw">c</span>(), <span class="kw">c</span>(<span class="st">&quot;x&quot;</span>,<span class="st">&quot;y&quot;</span>))), <span class="dt">stringsAsFactors =</span> F)
cells_df$x =<span class="st"> </span>cells$x
cells_df$y =<span class="st"> </span>cells$y

a2 =<span class="st"> </span><span class="kw">list</span>(<span class="dt">text =</span> <span class="st">&quot;Fig 1.2. Cells&quot;</span>, <span class="dt">showarrow =</span> <span class="ot">FALSE</span>, <span class="dt">xref =</span> <span class="st">&quot;paper&quot;</span>, <span class="dt">yref =</span> <span class="st">&quot;paper&quot;</span>, <span class="dt">yanchor =</span> <span class="st">&quot;bottom&quot;</span>,
  <span class="dt">xanchor =</span> <span class="st">&quot;center&quot;</span>, <span class="dt">align =</span> <span class="st">&quot;center&quot;</span>, <span class="dt">x =</span> <span class="fl">0.5</span>, <span class="dt">y =</span> <span class="dv">1</span>)
p2 =<span class="st"> </span><span class="kw">plot_ly</span>(cells_df, <span class="dt">x =</span> ~x, <span class="dt">y =</span> ~y) %&gt;%<span class="st"> </span><span class="kw">layout</span>(<span class="dt">annotations =</span> a2, <span class="dt">xaxis =</span> xlabs, <span class="dt">yaxis =</span> ylabs)

p =<span class="st"> </span><span class="kw">subplot</span>(p1,p2,<span class="dt">titleX =</span> <span class="ot">TRUE</span>, <span class="dt">titleY =</span> <span class="ot">TRUE</span>) %&gt;%<span class="st"> </span><span class="kw">layout</span>(<span class="dt">showlegend =</span> <span class="ot">FALSE</span>)
p</code></pre></div>
<div id="2a7cf74ddc" style="width:864px;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="2a7cf74ddc">{"x":{"data":[{"x":[0.36,0.44,0.48,0.48,0.5,0.76,0.78,0.78,0.84,0.86,0.9,0.9,0.9,0.9,0.18,0.18,0.2,0.6,0.62,0.64,0.64,0.68,0.68,0.7,0.72,0.1,0.12,0.12,0.14,0.18,0.18,0.2,0.2,0.22,0.24,0.52,0.5,0.58,0.94,0.96,0.22,0.22,0.26,0.44,0.48,0.5,0.9,0.9,0.48,0.52,0.56,0.999,0.44,0.48,0.34,0.38,0.26,0.4,0.28,0.74,0.86,0.96],"y":[-0.08,-0.1,-0.08,-0.14,-0.1,-0.14,-0.12,-0.16,-0.08,-0.18,-0.08,-0.1,-0.16,-0.2,-0.4,-0.38,-0.42,-0.34,-0.34,-0.36,-0.28,-0.32,-0.24,-0.28,-0.26,-0.58,-0.58,-0.62,-0.58,-0.56,-0.54,-0.52,-0.5,-0.46,-0.48,-0.58,-0.6,-0.58,-0.52,-0.54,-0.8,-0.84,-0.7,-0.76,-0.78,-0.76,-0.76,-0.78,-0.68,-0.66,-0.64,-0.84,-0.82,-0.82,-0.84,-0.84,-0.86,-0.86,-0.86,-0.9,-0.9,-0.96],"type":"scatter","mode":"markers","marker":{"fillcolor":"rgba(31,119,180,1)","color":"rgba(31,119,180,1)","line":{"color":"transparent"}},"xaxis":"x","yaxis":"y","frame":null},{"x":[0.35,0.487,0.637,0.775,0.825,0.087,0.237,0.4,0.575,0.737,0.062,0.212,0.325,0.45,0.65,0.9,0.337,0.462,0.6,0.8,0.938,0.15,0.35,0.562,0.725,0.862,0.987,0.062,0.175,0.337,0.462,0.525,0.737,0.862,0.237,0.637,0.775,0.9,0.175,0.35,0.462,0.625],"y":[0.025,0.087,0.05,0.025,0.125,0.187,0.15,0.162,0.212,0.237,0.362,0.337,0.287,0.287,0.362,0.262,0.462,0.425,0.475,0.387,0.4,0.5,0.6,0.575,0.512,0.525,0.512,0.75,0.65,0.75,0.75,0.65,0.687,0.637,0.787,0.812,0.85,0.775,0.912,0.962,0.9,0.95],"type":"scatter","mode":"markers","marker":{"fillcolor":"rgba(31,119,180,1)","color":"rgba(255,127,14,1)","line":{"color":"transparent"}},"xaxis":"x2","yaxis":"y2","frame":null}],"layout":{"xaxis":{"domain":[0,0.48],"title":"","linecolor":"rgba(211,211,211,1)","zerolinecolor":"rgba(255,255,255,1)","anchor":"y"},"xaxis2":{"domain":[0.52,1],"title":"","linecolor":"rgba(211,211,211,1)","zerolinecolor":"rgba(255,255,255,1)","anchor":"y2"},"yaxis2":{"domain":[0,1],"title":"","linecolor":"rgba(211,211,211,1)","zerolinecolor":"rgba(255,255,255,1)","anchor":"x2"},"yaxis":{"domain":[0,1],"title":"","linecolor":"rgba(211,211,211,1)","zerolinecolor":"rgba(255,255,255,1)","anchor":"x"},"annotations":[{"text":"Fig 1.1. Redwood seedlings","showarrow":false,"xref":"paper","yref":"paper","yanchor":"bottom","xanchor":"center","align":"center","x":0.24,"y":1},{"text":"Fig 1.2. Cells","showarrow":false,"xref":"paper","yref":"paper","yanchor":"bottom","xanchor":"center","align":"center","x":0.76,"y":1}],"margin":{"b":40,"l":60,"t":25,"r":10},"hovermode":"closest","showlegend":false},"attrs":{"2a7c3b983af1":{"x":{},"y":{},"alpha":1,"sizes":[10,100]}},"source":"A","config":{"modeBarButtonsToAdd":[{"name":"Collaborate","icon":{"width":1000,"ascent":500,"descent":-50,"path":"M487 375c7-10 9-23 5-36l-79-259c-3-12-11-23-22-31-11-8-22-12-35-12l-263 0c-15 0-29 5-43 15-13 10-23 23-28 37-5 13-5 25-1 37 0 0 0 3 1 7 1 5 1 8 1 11 0 2 0 4-1 6 0 3-1 5-1 6 1 2 2 4 3 6 1 2 2 4 4 6 2 3 4 5 5 7 5 7 9 16 13 26 4 10 7 19 9 26 0 2 0 5 0 9-1 4-1 6 0 8 0 2 2 5 4 8 3 3 5 5 5 7 4 6 8 15 12 26 4 11 7 19 7 26 1 1 0 4 0 9-1 4-1 7 0 8 1 2 3 5 6 8 4 4 6 6 6 7 4 5 8 13 13 24 4 11 7 20 7 28 1 1 0 4 0 7-1 3-1 6-1 7 0 2 1 4 3 6 1 1 3 4 5 6 2 3 3 5 5 6 1 2 3 5 4 9 2 3 3 7 5 10 1 3 2 6 4 10 2 4 4 7 6 9 2 3 4 5 7 7 3 2 7 3 11 3 3 0 8 0 13-1l0-1c7 2 12 2 14 2l218 0c14 0 25-5 32-16 8-10 10-23 6-37l-79-259c-7-22-13-37-20-43-7-7-19-10-37-10l-248 0c-5 0-9-2-11-5-2-3-2-7 0-12 4-13 18-20 41-20l264 0c5 0 10 2 16 5 5 3 8 6 10 11l85 282c2 5 2 10 2 17 7-3 13-7 17-13z m-304 0c-1-3-1-5 0-7 1-1 3-2 6-2l174 0c2 0 4 1 7 2 2 2 4 4 5 7l6 18c0 3 0 5-1 7-1 1-3 2-6 2l-173 0c-3 0-5-1-8-2-2-2-4-4-4-7z m-24-73c-1-3-1-5 0-7 2-2 3-2 6-2l174 0c2 0 5 0 7 2 3 2 4 4 5 7l6 18c1 2 0 5-1 6-1 2-3 3-5 3l-174 0c-3 0-5-1-7-3-3-1-4-4-5-6z"},"click":"function(gd) { \n        // is this being viewed in RStudio?\n        if (location.search == '?viewer_pane=1') {\n          alert('To learn about plotly for collaboration, visit:\\n https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html');\n        } else {\n          window.open('https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html', '_blank');\n        }\n      }"}],"cloud":false},"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1}},"subplot":true,"base_url":"https://plot.ly"},"evals":["config.modeBarButtonsToAdd.0.click"],"jsHooks":{"render":[{"code":"function(el, x) { var ctConfig = crosstalk.var('plotlyCrosstalkOpts').set({\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1}}); }","data":null}]}}</script>
<p><br></p>
<p>Consider the 2 datasets above. The one on the left represents the locations of redwood seedlings in a section of a forest. Intuitively, we would say this dataset is <strong>clustered</strong>. On the other hand, the right graph shows the locations of the centres of 42 biological cells observed under optical microscopy. In this case, we would be more inclined to say that these cells are <strong>dispersed</strong>. Now, these assumptions of datasets being clustered or dispersed come with the interpretation of each one of us as humans, so a very logical question would be: <em>are there any methods that can help us distinguish between clustered and dispersed datasets in a statistical and scientific way?</em></p>
<p>The answer is YES. The basis of all methods that we are going to present is the comparison of, a certain distribution/probability against a theoretical random distribution of points. The theoretical distribution will act as the benchmark against which we can compare via functions/p-values/etc… our empirical calculations.</p>
<p><br></p>
</div>
</div>
<div id="models-of-spatial-randomness" class="section level1">
<h1><span class="header-section-number">2</span> Models of Spatial Randomness</h1>
<p><br></p>
<p>As with most statistical analyses, cluster analysis of point patterns begins by asking: <em>What would point patterns look like if points were randomly distributed?</em> This requires a statistical model of randomly located points.</p>
<p><br></p>
<div id="spatial-laplace-principle" class="section level2">
<h2><span class="header-section-number">2.1</span> Spatial Laplace Principle</h2>
<p><br></p>
<p>Spatial Laplace Principle is very easy to understand. It basically says that, if there is not available information that indicates if certain events that we are studying are more likely than others, then we should basically treat all events the same and therefore say that they have the same probability of occuring. Translating this into a graphical explanation, if we have an area divided in equal areas, there is no reason to believe that this point is more likely to appear in either left half or the (identical) right half. If we look at the image below, for the first case, any given point should have the same probability (1/2) of appearing in either half of the area. If we divide the areas again by half, then points should have the same probability (1/4) of appearing in any of the 4 squares. Etc, etc, etc…</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">include_graphics</span>(<span class="kw">paste0</span>(source_path,<span class="st">&quot;/images/1_Spatial_Laplace_Principle.PNG&quot;</span>))</code></pre></div>
<p><img src="images/1_Spatial_Laplace_Principle.PNG" width="635" /></p>
<p><br></p>
<p>To put this into mathematical terms, the probability that a random point in <span class="math inline">\(S\)</span> lies in any cell <span class="math inline">\(C \subset S\)</span> is proportional to the area of <span class="math inline">\(C\)</span>.</p>
<p><span class="math display">\[ Pr(C|S) = \frac{a(C)}{a(S)} \]</span></p>
<p><br></p>
</div>
<div id="complete-spatial-randomness-csr" class="section level2">
<h2><span class="header-section-number">2.2</span> Complete Spatial Randomness (CSR)</h2>
<p><br></p>
<p>Having understood that the first assumption of spatial randomness is that, without any given information on the likehood of events occuring being different across the dataset, the probability is should be the same for all events; then we move to the second key assumption. This second assumption considers that that the locations of these points have no influence on one another. For example, animal packs location tend not to be randomnly spaced because each pack has it’s own hunting areas, and hence animal packs location will have influence against other packs (breaking the second key assumption).</p>
<p>This together with the Spatial Laplace Principle above defines the fundamental hypothesis of complete spatial randomness (CSR), which we shall usually refer to as the CSR Hypothesis.</p>
<p><br></p>
<div id="for-small-regions" class="section level3">
<h3><span class="header-section-number">2.2.1</span> For small regions</h3>
<p><br></p>
<p>To put this in mathematical terms for small regions, we consider the total number of points appearing in <span class="math inline">\(C\)</span>:</p>
<p><span class="math display">\[ N(C) = \sum_{i=1}^n X_i(C),  \quad\quad with\:X_i=1\:for\:points\:inside\:C,\:0\:otherwise \]</span> After this, if we would like to compare any dataset to this CSR hypothesis, we would need to compare it against “something”. Remember that this something for the Spatial Laplace Principle was a simple as comparing the proportion of points per area against the theoretical <span class="math inline">\(1/n\)</span>. For CSR, we compare against a distribution.</p>
<p>We can use Bernouilli indepedent variables and build a Binomial random variable distribution:</p>
<p><span class="math display">\[PR[N(C) = k|n,R] = \frac{n!}{k!(n-k)!}\bigg(\frac{a(C)}{a(R)}\bigg)^k\bigg(1-\frac{a(C)}{a(R)}\bigg)^{n-k}, k = 0,1,...,n\]</span></p>
<p>Messy formula eh! Anyway, this is not the distribution we will normally use, so don’t worry too much about it. But if we would, we could just use a statisticall package in R and calculate the theoretical Binomial distribution for number of points we have in our spatial dataset.</p>
<p><br></p>
</div>
<div id="for-large-regions" class="section level3">
<h3><span class="header-section-number">2.2.2</span> For large regions</h3>
<p><br></p>
<p>If the reference region <span class="math inline">\(R\)</span> is large, then the exact specification of the area and number of points in contains is not of that much interest and we can remove these conditioning effects by studying the Poission approximation instead of the Binomial distribution. YOu might ask, well, why is the area of study important in using other distributions? Imagine yourself in Tiananmen square and it starts to rain. You have a magical machine that counts the number of drops that are falling where you are standing. Would that count change if the size of the square was bigger or smaller? No. Would that count depend on the total number of drops hitting the whole square? No. Hence the specification of the area itself (shape of the square) and total number of points in the area is not that interesting.</p>
<p>What would be very interesting for us is to know the <strong>intensity</strong> of the rainfall! In the square, we should expect suddenly having in a single square tile an intensity of 1000 drops per tile and just right next to it only 1 drop per tile. Intensity might vary throughtout the whole square, but it shouldnt vary that much, and if it does, it should be smooth variations. Therefore, we want to introduce the concept of intensity to our CSR hypothesis.</p>
<p>Therefore we define the <strong>expected density</strong> in such a way that we allow the area and the number of points to vary but keeping the intensity the same:</p>
<p><span class="math display">\[ \lambda(n,R) = \frac{n}{a(R)}\]</span></p>
<p>Using this we can define the expected number of points in any given cell:</p>
<p><span class="math display">\[ E[N(C)] = \lambda a(C)\]</span></p>
<p>Under the above conditions, the Binomial probabilities used in small areas, converge to simple Poisson probabilities:</p>
<p><span class="math display">\[ PR[N(C) = k|n,R] = \frac{[\lambda a(C)]}{k!}^k e^{-\lambda a(C)} \]</span></p>
<p><br></p>
</div>
</div>
</div>
<div id="testing-csr---basics" class="section level1">
<h1><span class="header-section-number">3</span> Testing CSR - Basics</h1>
<p><br></p>
<p>I know that the previous sectin was a bit tedious with formulas and mathematical terms. But it was necessary to understand the basis of everything that we are going to test. We can summarise it by saying that, we want to test certian spatial dataset against a theoretical distribution or theoretical pattern, which can be given by the simplest Spatial Laplace Principle to a bit more elaborated Poisson distribution.</p>
<p>In this section we are going to discuss 3 different methods that build one after the other using the above concepts. These 3 approaches to test the CSR hypothesis are: * Quadrat Method * Nearest Neighbour Method * Method of K-functions</p>
<p>You will how we introduce these methods bit by bit with examples and normally conclude with disadvantages or problems we might find whilst using them, which leads to the following testing methodology. Let’s start!</p>
<p><br></p>
<div id="quadrat-method" class="section level2">
<h2><span class="header-section-number">3.1</span> Quadrat method</h2>
<p><br></p>
<p>This simple method is essentially a direct test of the CSR Hypothesis as stated in the Spatial Laplace Principle, where given a point pattern in a <em>rectangular</em> region <span class="math inline">\(R\)</span>, one begins by partitioning <span class="math inline">\(R\)</span> it into rectangular subcells of equal area, and the CSR hypothesis says that the cell-count distribution for each subcell must be the same. In other words, the distribution or count of subcells which 0 points, or 1 point, or 2 points, etc, should be the same across the study area. If we assume the area <span class="math inline">\(R\)</span> is large enough, then we can use the Poisson approximation and the point density as defined above.</p>
<p>I hope you remember your statistics a little bit… Basically, having the above, the easiest way to test the CSR hypothesis is to use the Pearson <span class="math inline">\(\chi^{2}\)</span> goodness-of-fit test. How does the Pearson <span class="math inline">\(\chi^{2}\)</span> goodness-of-fit test work? I will explain the steps, then implement a very easy to follow R code to show roughly how it works, and then use a proper <em>R</em> library (obviously using various examples).</p>
<p><br></p>
<div id="the-pearson-chi2-goodness-of-fit-test" class="section level3">
<h3><span class="header-section-number">3.1.1</span> The Pearson <span class="math inline">\(\chi^{2}\)</span> goodness-of-fit test:</h3>
<ul>
<li>Having our spatial dataset, divide the whole area into smaller equal rectangular subcells</li>
<li>Count the number of points in each subcell and record all the counted combinations. For example, if we have that 10 subcells have no points, 3 subcells have 1 point and 2 subcells have 2 points, we know that our possible outcomes are subcells with 0, 1, 2 points in them.</li>
<li>Used the counts to record the total number of cells with these possible outcomes (ie the same as above, 10 with no point, 3 with 1 point, 2 with 2 points).</li>
<li>Use the Poisson approximation with the mean expected number of points to calculate the theoretical outcomes that we would have. For example, (and I am making this up), theoretically under CSR, we should have 5 subcells with 0 points, 3 subcells with 1 point, 3 subcells with 2 points and 1 subcell with 3 points.</li>
</ul>
<p><span class="math display">\[ E(N|\lambda) = a \cdot \lambda = a \cdot \frac{n}{a(R)} \]</span></p>
<ul>
<li>Calculate the differences between the observed and the theoretical possible outcomes.</li>
<li>Calculate the individual <span class="math inline">\(\chi^{2}\)</span> statistic by dividing the difference by the expected Poisson outcome</li>
<li>Sum all of the individual <span class="math inline">\(\chi^{2}\)</span> statistics to get the global one</li>
<li>Finally, check the theoretical <span class="math inline">\(\chi^{2}\)</span> value found in tables with <span class="math inline">\(b-1\)</span> degrees of freedom, being <span class="math inline">\(b\)</span> the total number of different bins.</li>
</ul>
<p><span class="math display">\[ \chi^{2} = \sum_{i=1}^n \frac{n_i-n/m}{n/m}^2 \]</span></p>
<ul>
<li>Our statistical test will be:
<ul>
<li>NULL HYPOTHESIS. We assume that there is no significant difference between the observed and the expected value. In other words, if the expected <span class="math inline">\(\chi^2\)</span> (given a certain probability threshold) is bigger than the observed <span class="math inline">\(\chi^2\)</span>, the p-value would be bigger than that probability threshold. Hence we retain <span class="math inline">\(H_0\)</span> and conclude that there is insufficient evidence to suggest that the spatial distribution does not follow a Poisson distribution and therefore, is not random.</li>
<li>ALTERNATIVE HYPOTHESIS. Obviously, this will test the opposite, that there is a significant difference (i.e. the expected <span class="math inline">\(\chi^2\)</span> (given a certain probability threshold) is smaller than the observed <span class="math inline">\(\chi^2\)</span>). If there is a significant we would be rejecting the null hypothesis and would say that the observed distribution does not follow a random spatial distribution (it can be regular or aggregated).</li>
<li>To check this hypothesis we need to calculate the actual <span class="math inline">\(\chi^2\)</span> value of the whole distribution and compare it against a theoretical <span class="math inline">\(\chi^2\)</span> value. This theoretical value can be calculated using <span class="math inline">\(\chi^2\)</span> tables.</li>
</ul></li>
</ul>
<p><br></p>
</div>
<div id="creating-pearson-chi2-goodness-of-fit-test-function" class="section level3">
<h3><span class="header-section-number">3.1.2</span> Creating Pearson <span class="math inline">\(\chi^{2}\)</span> goodness-of-fit test function</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># --------------------------------------------------------------------------------------------------------</span>
<span class="co"># Function to perform the count of points per quadrat and produce a scatter plot and a histogram</span>
<span class="co"># --------------------------------------------------------------------------------------------------------</span>
scatter_hist_pointdf =<span class="st"> </span>function(x,y,squares_grid){
    
    <span class="co"># Create a datafarme with the coordinates of the points we are going to plot</span>
    df =<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> y)
    
    <span class="co"># We use squares_grid to define how many subcells we want to divide our axis in. If we want 4 squares across the x and y axis, then squares_grid = 4, which will divide our whole area in 16 squares. </span>
    <span class="co"># We use this to create a numeric sequence that will be used to placed our break points (subcells) in our graph</span>
    nsquares =<span class="st"> </span>squares_grid
    
    if(<span class="kw">round</span>(<span class="kw">max</span>(x))/nsquares&gt;<span class="dv">0</span>){breaks_x =<span class="st"> </span><span class="kw">seq</span>(<span class="kw">round</span>(<span class="kw">min</span>(x)),<span class="kw">round</span>(<span class="kw">max</span>(x)),<span class="kw">abs</span>(<span class="kw">round</span>(<span class="kw">max</span>(x))/nsquares))}
    if(<span class="kw">round</span>(<span class="kw">max</span>(x))/nsquares&lt;<span class="dv">0</span>){breaks_x =<span class="st"> </span><span class="kw">seq</span>(<span class="kw">round</span>(<span class="kw">min</span>(x)),<span class="kw">round</span>(<span class="kw">max</span>(x)),<span class="kw">abs</span>(<span class="kw">round</span>(<span class="kw">min</span>(x))/nsquares))}
    if(<span class="kw">round</span>(<span class="kw">max</span>(x))/nsquares==<span class="dv">0</span>){breaks_x =<span class="st"> </span><span class="kw">seq</span>(<span class="kw">round</span>(<span class="kw">min</span>(x))-<span class="dv">1</span>,<span class="kw">round</span>(<span class="kw">max</span>(x)),<span class="fl">0.1</span>)}
    
    if(<span class="kw">round</span>(<span class="kw">max</span>(y))/nsquares&gt;<span class="dv">0</span>){breaks_y =<span class="st"> </span><span class="kw">seq</span>(<span class="kw">round</span>(<span class="kw">min</span>(y)),<span class="kw">round</span>(<span class="kw">max</span>(y)),<span class="kw">abs</span>(<span class="kw">round</span>(<span class="kw">max</span>(y))/nsquares))}
    if(<span class="kw">round</span>(<span class="kw">max</span>(y))/nsquares&lt;<span class="dv">0</span>){breaks_y =<span class="st"> </span><span class="kw">seq</span>(<span class="kw">round</span>(<span class="kw">min</span>(y)),<span class="kw">round</span>(<span class="kw">max</span>(y)),<span class="kw">abs</span>(<span class="kw">round</span>(<span class="kw">min</span>(y))/nsquares))}
    if(<span class="kw">round</span>(<span class="kw">max</span>(y))/nsquares==<span class="dv">0</span>){breaks_y =<span class="st"> </span><span class="kw">seq</span>(<span class="kw">round</span>(<span class="kw">min</span>(y)),<span class="kw">round</span>(<span class="kw">max</span>(y)),<span class="fl">0.1</span>)}
    
    <span class="kw">print</span>(breaks_x)
    <span class="kw">print</span>(breaks_y)
    
    <span class="co"># This piece of code will just plot the points with the x, y coordinates and divide the area using the break points defined above</span>
    p1 =<span class="st"> </span><span class="kw">ggplot</span>(df, <span class="kw">aes</span>(<span class="dt">x =</span> df$x, <span class="dt">y =</span> df$y)) +<span class="st"> </span><span class="kw">geom_point</span>(<span class="dt">col =</span> <span class="st">&quot;green&quot;</span>)
    p1 =<span class="st"> </span>p1 +<span class="st"> </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;&quot;</span>, <span class="dt">y =</span> <span class="st">&quot;&quot;</span>)
    p1 =<span class="st"> </span>p1 +<span class="st"> </span><span class="kw">theme_minimal</span>()
    p1 =<span class="st"> </span>p1 +<span class="st"> </span><span class="kw">theme</span>(<span class="dt">panel.grid.major.y =</span> <span class="kw">element_line</span>(<span class="dt">colour =</span> <span class="st">&quot;grey&quot;</span>, <span class="dt">size =</span> <span class="fl">0.5</span>)
                    , <span class="dt">panel.grid.minor.y =</span> <span class="kw">element_line</span>(<span class="dt">colour =</span> <span class="st">&quot;grey&quot;</span>, <span class="dt">size =</span> <span class="fl">0.5</span>)
                    , <span class="dt">panel.grid.major.x =</span> <span class="kw">element_line</span>(<span class="dt">colour =</span> <span class="st">&quot;grey&quot;</span>, <span class="dt">size =</span> <span class="fl">0.5</span>)
                    , <span class="dt">panel.grid.minor.x =</span> <span class="kw">element_line</span>(<span class="dt">colour =</span> <span class="st">&quot;grey&quot;</span>, <span class="dt">size =</span> <span class="fl">0.5</span>))
    p1 =<span class="st"> </span>p1 +<span class="st"> </span><span class="kw">scale_x_continuous</span>(<span class="dt">breaks =</span> breaks_x) +<span class="st"> </span><span class="kw">scale_y_continuous</span>(<span class="dt">breaks =</span> breaks_y)
    p1 =<span class="st"> </span><span class="kw">ggplotly</span>(p1)
    
    
    <span class="co"># Count the number of points per grid</span>
    <span class="co"># ------------------------------------</span>
    <span class="co">#   - The trick is to use the function cut(). The documentation about cut() says that it divides the range of x into intervals and codes the values in x according to which interval they fall. The leftmost interval corresponds to level one, the next leftmost to level two and so on.</span>
    <span class="co">#   - Basically, it is converting the x and y coordinates into bin numbers. In other words, we will cut the xaxis at break points = xmax/nsquares (if we had created a grid with 10 squares in an axis in a range of 0 to 5, then this would equal to 0.5), then, we will create bins of length xmax/nsquares (i.e. (0,0.5],(0.5,1],etc))).</span>
    xcut =<span class="st"> </span><span class="kw">cut</span>(x, breaks_x)
    ycut =<span class="st"> </span><span class="kw">cut</span>(y, breaks_y)
    
    <span class="co"># Once we have created our bins, we count the points that fall within them.</span>
    count_matrix =<span class="st"> </span><span class="kw">table</span>(ycut,xcut)
    points_in =<span class="st"> </span><span class="kw">as.vector</span>(count_matrix)

    <span class="co">#print(count_matrix)</span>
    
    <span class="co">#print(points_in)</span>
    
    <span class="co"># Finally, we count the number of times each frequency appears.</span>
    freq_count =<span class="st"> </span><span class="kw">as.data.frame</span>(<span class="kw">table</span>(points_in))
    
    <span class="co">#print(freq_count)</span>
    
    freq_count$points_in =<span class="st"> </span><span class="kw">as.factor</span>(freq_count$points_in)
    
    <span class="co"># We create a plot of the bar graph/histogram of bins and frequency of subcells that contain that number of points.</span>
    p2 =<span class="st"> </span><span class="kw">plot_ly</span>(<span class="dt">x =</span> freq_count$points_in
                , <span class="dt">y =</span> freq_count$Freq
                , <span class="dt">type =</span> <span class="st">&quot;bar&quot;</span>)
    
    p =<span class="st"> </span><span class="kw">subplot</span>(p1,p2,<span class="dt">margin =</span> <span class="fl">0.05</span>)
    
    <span class="co"># Return the frequency count and the plot object for later use.</span>
    <span class="kw">return</span>(<span class="kw">list</span>(freq_count,p))
}

<span class="co"># --------------------------------------------------------------------------------------------------------</span>
<span class="co"># Function to perform the poisson and chi-squared calculations</span>
<span class="co"># --------------------------------------------------------------------------------------------------------</span>
poisson_chi_calc =<span class="st"> </span>function(N,freq_count){
    
    <span class="co"># Build all possible bins from 0 to the maximum number of points in a subcell</span>
    bins_complete =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>
                        , <span class="kw">max</span>(<span class="kw">as.numeric</span>(<span class="kw">as.character</span>(freq_count$points_in)))
                        , <span class="dv">1</span>)
    
    <span class="co"># Ensure the point_in parameter is in a numeric class</span>
    freq_count$points_in =<span class="st"> </span><span class="kw">as.numeric</span>(<span class="kw">as.character</span>(freq_count$points_in))
    
    <span class="co"># Start building the final table. We start by adding all possible bins, and the joining the corresponding observed count frequencies to the corresponding bins. Remember that in the observed pattern we might have 1000 subcells with 0 points and 10 subcells with 10 points, with no intermediate bins in between. However, the theoretical distribution might have all bins going from 0 to 10 points.</span>
    bins_final =<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">bins_complete =</span> bins_complete)
    bins_final =<span class="st"> </span><span class="kw">merge</span>(<span class="dt">x =</span> bins_final, <span class="dt">y =</span> freq_count
                            , <span class="dt">by.x =</span> <span class="st">&quot;bins_complete&quot;</span>, <span class="dt">by.y =</span> <span class="st">&quot;points_in&quot;</span>, <span class="dt">all.x =</span> <span class="ot">TRUE</span>)
    bins_final$Freq =<span class="st"> </span><span class="kw">ifelse</span>(<span class="kw">is.na</span>(bins_final$Freq),<span class="dv">0</span>,bins_final$Freq)
    
    <span class="co"># Calculating the expected poisson distribution values -&gt; we use the dpois function from the stat package</span>
    <span class="co"># -----------------------------------------------------------------------------------------------</span>
    
    <span class="co"># We calculate average intensity across the whole spatial dataset (total number of points by total number of subcells (nsquares^2 because nsquares represents only the number of subcells in a single axis))</span>
    N =<span class="st"> </span>N
    mean =<span class="st"> </span>N/nsquares^<span class="dv">2</span>
    
    <span class="kw">print</span>(<span class="kw">paste</span>(<span class="st">&quot;Mean of number points per square: &quot;</span>,mean))
    
    <span class="co"># Dpois function to calculate expected number of points</span>
    expected_Poisson =<span class="st"> </span><span class="dv">100</span>*<span class="kw">dpois</span>(bins_final$bins_complete, <span class="dt">lambda =</span> mean, <span class="dt">log =</span> <span class="ot">FALSE</span>)
    
    <span class="co"># Setting table to easily visualise calculations</span>
    chi_table =<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">bins =</span> bins_final$bins_complete
                           , <span class="dt">observed =</span> bins_final$Freq
                           , <span class="dt">expected =</span> expected_Poisson)
    
    <span class="co"># Difference between the observed and the expeceted distributions</span>
    chi_table$diff_obs_exp =<span class="st"> </span><span class="kw">abs</span>(chi_table$observed -<span class="st"> </span>chi_table$expected)
    
    <span class="co"># Individual chi squares</span>
    chi_table$indv_chi_squared =<span class="st"> </span>(chi_table$diff_obs_exp^<span class="dv">2</span>)/(chi_table$expected)
    
    <span class="co"># Adding the last row, which represent the final chi-square statistic as the sum of all the individual chi-square values.</span>
    <span class="kw">rbind</span>(chi_table,<span class="kw">c</span>(<span class="st">&quot;Sum&quot;</span>,<span class="kw">sum</span>(chi_table$observed),<span class="kw">sum</span>(chi_table$expected),<span class="kw">sum</span>(chi_table$diff_obs_exp),<span class="kw">sum</span>(chi_table$indv_chi_squared)))  
}</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">nsquares =<span class="st"> </span><span class="dv">10</span>
N =<span class="st"> </span>redwood$n

<span class="co"># Counting points per bins</span>
result =<span class="st"> </span><span class="kw">scatter_hist_pointdf</span>(<span class="dt">x =</span> redwood_df$x, <span class="dt">y =</span> redwood_df$y, nsquares)</code></pre></div>
<pre><code>##  [1] 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
##  [1] -1.0 -0.9 -0.8 -0.7 -0.6 -0.5 -0.4 -0.3 -0.2 -0.1  0.0</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># freq_points</span>
freq_points =<span class="st"> </span>result[[<span class="dv">1</span>]]

<span class="co"># Plot</span>
result[[<span class="dv">2</span>]]</code></pre></div>
<div id="2a7c34c67f55" style="width:864px;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="2a7c34c67f55">{"x":{"data":[{"x":[0.36,0.44,0.48,0.48,0.5,0.76,0.78,0.78,0.84,0.86,0.9,0.9,0.9,0.9,0.18,0.18,0.2,0.6,0.62,0.64,0.64,0.68,0.68,0.7,0.72,0.1,0.12,0.12,0.14,0.18,0.18,0.2,0.2,0.22,0.24,0.52,0.5,0.58,0.94,0.96,0.22,0.22,0.26,0.44,0.48,0.5,0.9,0.9,0.48,0.52,0.56,0.999,0.44,0.48,0.34,0.38,0.26,0.4,0.28,0.74,0.86,0.96],"y":[-0.08,-0.1,-0.08,-0.14,-0.1,-0.14,-0.12,-0.16,-0.08,-0.18,-0.08,-0.1,-0.16,-0.2,-0.4,-0.38,-0.42,-0.34,-0.34,-0.36,-0.28,-0.32,-0.24,-0.28,-0.26,-0.58,-0.58,-0.62,-0.58,-0.56,-0.54,-0.52,-0.5,-0.46,-0.48,-0.58,-0.6,-0.58,-0.52,-0.54,-0.8,-0.84,-0.7,-0.76,-0.78,-0.76,-0.76,-0.78,-0.68,-0.66,-0.64,-0.84,-0.82,-0.82,-0.84,-0.84,-0.86,-0.86,-0.86,-0.9,-0.9,-0.96],"text":["df$x: 0.360<br />df$y: -0.08","df$x: 0.440<br />df$y: -0.10","df$x: 0.480<br />df$y: -0.08","df$x: 0.480<br />df$y: -0.14","df$x: 0.500<br />df$y: -0.10","df$x: 0.760<br />df$y: -0.14","df$x: 0.780<br />df$y: -0.12","df$x: 0.780<br />df$y: -0.16","df$x: 0.840<br />df$y: -0.08","df$x: 0.860<br />df$y: -0.18","df$x: 0.900<br />df$y: -0.08","df$x: 0.900<br />df$y: -0.10","df$x: 0.900<br />df$y: -0.16","df$x: 0.900<br />df$y: -0.20","df$x: 0.180<br />df$y: -0.40","df$x: 0.180<br />df$y: -0.38","df$x: 0.200<br />df$y: -0.42","df$x: 0.600<br />df$y: -0.34","df$x: 0.620<br />df$y: -0.34","df$x: 0.640<br />df$y: -0.36","df$x: 0.640<br />df$y: -0.28","df$x: 0.680<br />df$y: -0.32","df$x: 0.680<br />df$y: -0.24","df$x: 0.700<br />df$y: -0.28","df$x: 0.720<br />df$y: -0.26","df$x: 0.100<br />df$y: -0.58","df$x: 0.120<br />df$y: -0.58","df$x: 0.120<br />df$y: -0.62","df$x: 0.140<br />df$y: -0.58","df$x: 0.180<br />df$y: -0.56","df$x: 0.180<br />df$y: -0.54","df$x: 0.200<br />df$y: -0.52","df$x: 0.200<br />df$y: -0.50","df$x: 0.220<br />df$y: -0.46","df$x: 0.240<br />df$y: -0.48","df$x: 0.520<br />df$y: -0.58","df$x: 0.500<br />df$y: -0.60","df$x: 0.580<br />df$y: -0.58","df$x: 0.940<br />df$y: -0.52","df$x: 0.960<br />df$y: -0.54","df$x: 0.220<br />df$y: -0.80","df$x: 0.220<br />df$y: -0.84","df$x: 0.260<br />df$y: -0.70","df$x: 0.440<br />df$y: -0.76","df$x: 0.480<br />df$y: -0.78","df$x: 0.500<br />df$y: -0.76","df$x: 0.900<br />df$y: -0.76","df$x: 0.900<br />df$y: -0.78","df$x: 0.480<br />df$y: -0.68","df$x: 0.520<br />df$y: -0.66","df$x: 0.560<br />df$y: -0.64","df$x: 0.999<br />df$y: -0.84","df$x: 0.440<br />df$y: -0.82","df$x: 0.480<br />df$y: -0.82","df$x: 0.340<br />df$y: -0.84","df$x: 0.380<br />df$y: -0.84","df$x: 0.260<br />df$y: -0.86","df$x: 0.400<br />df$y: -0.86","df$x: 0.280<br />df$y: -0.86","df$x: 0.740<br />df$y: -0.90","df$x: 0.860<br />df$y: -0.90","df$x: 0.960<br />df$y: -0.96"],"type":"scatter","mode":"markers","marker":{"autocolorscale":false,"color":"rgba(0,255,0,1)","opacity":1,"size":5.66929133858268,"symbol":"circle","line":{"width":1.88976377952756,"color":"rgba(0,255,0,1)"}},"hoveron":"points","showlegend":false,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":["0","1","2","3","4","6"],"y":[69,13,9,7,1,1],"type":"bar","marker":{"fillcolor":"rgba(31,119,180,1)","color":"rgba(255,127,14,1)","line":{"color":"transparent"}},"xaxis":"x2","yaxis":"y2","frame":null}],"layout":{"xaxis":{"domain":[0,0.45],"type":"linear","autorange":false,"tickmode":"array","range":[0.05505,1.04395],"ticktext":["0.1","0.2","0.3","0.4","0.5","0.6","0.7","0.8","0.9","1.0"],"tickvals":[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1],"ticks":"","tickcolor":null,"ticklen":3.65296803652968,"tickwidth":0,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"","size":11.689497716895},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(190,190,190,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"y","titlefont":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187},"hoverformat":".2f"},"xaxis2":{"domain":[0.55,1],"type":"category","categoryorder":"array","categoryarray":["0","1","2","3","4","6"],"anchor":"y2"},"yaxis2":{"domain":[0,1],"anchor":"x2"},"yaxis":{"domain":[0,1],"type":"linear","autorange":false,"tickmode":"array","range":[-1.004,-0.036],"ticktext":["-1.0","-0.9","-0.8","-0.7","-0.6","-0.5","-0.4","-0.3","-0.2","-0.1"],"tickvals":[-1,-0.9,-0.8,-0.7,-0.6,-0.5,-0.4,-0.3,-0.2,-0.1],"ticks":"","tickcolor":null,"ticklen":3.65296803652968,"tickwidth":0,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"","size":11.689497716895},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(190,190,190,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"x","titlefont":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187},"hoverformat":".2f"},"shapes":[{"type":"rect","fillcolor":null,"line":{"color":null,"width":0,"linetype":[]},"yref":"paper","xref":"paper","x0":0,"x1":0.45,"y0":0,"y1":1}],"margin":{"t":25,"r":10,"b":40,"l":60},"font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187},"showlegend":false,"legend":{"bgcolor":null,"bordercolor":null,"borderwidth":0,"font":{"color":"rgba(0,0,0,1)","family":"","size":11.689497716895}},"hovermode":"closest"},"attrs":{"2a7c4aef3637":{"x":{},"y":{},"type":"ggplotly"},"2a7c770d30e9":{"x":["0","1","2","3","4","6"],"y":[69,13,9,7,1,1],"alpha":1,"sizes":[10,100],"type":"bar"}},"source":"A","config":{"modeBarButtonsToAdd":[{"name":"Collaborate","icon":{"width":1000,"ascent":500,"descent":-50,"path":"M487 375c7-10 9-23 5-36l-79-259c-3-12-11-23-22-31-11-8-22-12-35-12l-263 0c-15 0-29 5-43 15-13 10-23 23-28 37-5 13-5 25-1 37 0 0 0 3 1 7 1 5 1 8 1 11 0 2 0 4-1 6 0 3-1 5-1 6 1 2 2 4 3 6 1 2 2 4 4 6 2 3 4 5 5 7 5 7 9 16 13 26 4 10 7 19 9 26 0 2 0 5 0 9-1 4-1 6 0 8 0 2 2 5 4 8 3 3 5 5 5 7 4 6 8 15 12 26 4 11 7 19 7 26 1 1 0 4 0 9-1 4-1 7 0 8 1 2 3 5 6 8 4 4 6 6 6 7 4 5 8 13 13 24 4 11 7 20 7 28 1 1 0 4 0 7-1 3-1 6-1 7 0 2 1 4 3 6 1 1 3 4 5 6 2 3 3 5 5 6 1 2 3 5 4 9 2 3 3 7 5 10 1 3 2 6 4 10 2 4 4 7 6 9 2 3 4 5 7 7 3 2 7 3 11 3 3 0 8 0 13-1l0-1c7 2 12 2 14 2l218 0c14 0 25-5 32-16 8-10 10-23 6-37l-79-259c-7-22-13-37-20-43-7-7-19-10-37-10l-248 0c-5 0-9-2-11-5-2-3-2-7 0-12 4-13 18-20 41-20l264 0c5 0 10 2 16 5 5 3 8 6 10 11l85 282c2 5 2 10 2 17 7-3 13-7 17-13z m-304 0c-1-3-1-5 0-7 1-1 3-2 6-2l174 0c2 0 4 1 7 2 2 2 4 4 5 7l6 18c0 3 0 5-1 7-1 1-3 2-6 2l-173 0c-3 0-5-1-8-2-2-2-4-4-4-7z m-24-73c-1-3-1-5 0-7 2-2 3-2 6-2l174 0c2 0 5 0 7 2 3 2 4 4 5 7l6 18c1 2 0 5-1 6-1 2-3 3-5 3l-174 0c-3 0-5-1-7-3-3-1-4-4-5-6z"},"click":"function(gd) { \n        // is this being viewed in RStudio?\n        if (location.search == '?viewer_pane=1') {\n          alert('To learn about plotly for collaboration, visit:\\n https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html');\n        } else {\n          window.open('https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html', '_blank');\n        }\n      }"}],"cloud":false},"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1}},"subplot":true,"base_url":"https://plot.ly"},"evals":["config.modeBarButtonsToAdd.0.click"],"jsHooks":{"render":[{"code":"function(el, x) { var ctConfig = crosstalk.var('plotlyCrosstalkOpts').set({\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1}}); }","data":null}]}}</script>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Chi-square test</span>
<span class="kw">poisson_chi_calc</span>(N, freq_points)</code></pre></div>
<pre><code>## [1] &quot;Mean of number points per square:  0.62&quot;</code></pre>
<pre><code>##   bins observed            expected       diff_obs_exp   indv_chi_squared
## 1    0       69    53.7944437594674   15.2055562405326   4.29800783177179
## 2    1       13    33.3525551308698   20.3525551308698   12.4196331804187
## 3    2        9    10.3392920905696   1.33929209056964  0.173484150379929
## 4    3        7    2.13678703205106   4.86321296794894   11.0684125356773
## 5    4        1   0.331201989967914  0.668798010032086   1.35050752039929
## 6    5        0  0.0410690467560214 0.0410690467560214 0.0410690467560214
## 7    6        1 0.00424380149812221  0.995756198501878   233.642032336725
## 8  Sum      100      99.99959285118   43.4662396852109   262.993146602128</code></pre>
<p><br></p>
<p><strong>Interpretation of results</strong></p>
<p><br></p>
<p>Before looking up the tabular value for <span class="math inline">\(\chi^2\)</span> statistic, we could get a sense of what the result is going to be by looking at the differences between expected and observed. The total difference is ~49 which, compared to the total number of squares (100), is nearly 50%. Anyway, that is not really relevant. Let’s look at the final <span class="math inline">\(\chi^2\)</span> statistic:</p>
<ul>
<li>Our method has calculated a <span class="math inline">\(\chi^2\)</span> statistic of ~268.</li>
<li>In tabular form (look below - although it doesn’t cover up to 99 df), for a probability of 0.05 and degrees of freedom of 99 (100 squares - 1 (-1 because we can determine the number of points that will land on the last subcell by knowing the rest)), then we have 123.2252.</li>
<li>As 268 &gt;&gt; 123.2252, in other words, the observed is much bigger than the expected, we would be rejecting the null hypothesis and would say that the observed distribution does not follow a random spatial distribution (it can be regular or aggregated).</li>
<li>This makes sense because just looking at the spatial distribution of the redwood seedlings it does seem that clustering is happening.</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Calculating chi-square statistic with 0.05 conf interval</span>
<span class="kw">qchisq</span>(.<span class="dv">95</span>, <span class="dt">df=</span><span class="dv">99</span>)</code></pre></div>
<pre><code>## [1] 123.2252</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">include_graphics</span>(<span class="kw">paste0</span>(source_path,<span class="st">&quot;/images/2_chi-square_table.PNG&quot;</span>))</code></pre></div>
<p><img src="images/2_chi-square_table.PNG" width="804" /></p>
<p><br></p>
</div>
<div id="testing-with-r-functions" class="section level3">
<h3><span class="header-section-number">3.1.3</span> Testing with R functions</h3>
<p><br></p>
<p>Let’s repeat the same as above, but with in-built R functions.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(<span class="dt">x =</span> redwood$x, <span class="dt">y =</span> redwood$y, <span class="dt">pch=</span><span class="dv">16</span>,<span class="dt">cex=</span><span class="fl">0.5</span>, <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>
     , <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="kw">min</span>(redwood$y)-<span class="fl">0.1</span>,<span class="kw">max</span>(redwood$y)+<span class="fl">0.1</span>)
     , <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="kw">min</span>(redwood$x)-<span class="fl">0.1</span>,<span class="kw">max</span>(redwood$x)+<span class="fl">0.1</span>))
<span class="kw">par</span>(<span class="dt">new=</span><span class="ot">TRUE</span>)
<span class="kw">plot</span>(<span class="kw">quadratcount</span>(redwood,<span class="dt">nx =</span> nsquares, <span class="dt">ny =</span> nsquares),<span class="dt">add=</span>T)</code></pre></div>
<p><img src="index_files/figure-html/unnamed-chunk-36-1.png" /><!-- --></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">quadrat.test</span>(<span class="kw">quadratcount</span>(redwood,<span class="dt">nx =</span> nsquares, <span class="dt">ny =</span> nsquares))</code></pre></div>
<pre><code>## 
##  Chi-squared test of CSR using quadrat counts
##  Pearson X2 statistic
## 
## data:  
## X2 = 202.52, df = 99, p-value = 8.448e-09
## alternative hypothesis: two.sided
## 
## Quadrats: 10 by 10 grid of tiles</code></pre>
<p>Using in-built R function yields the same conclusion (although with some minor discrepancies in the <span class="math inline">\(\chi^2\)</span> statistic. To be fair that will probably be my fault when implement my own code. I think it has to do in the way we calculate the number of points that appear in each cell. With this I mean that, for example, the lower bounds of the x axis is open which means that if a point lands on that bound, it will not be counted or counted in the right square… Anyway, it doesn’t matter because that was just to show you some more visual graphs).</p>
<p>This <em>quadrat.test</em> function shows that, for a 10x10 grid, with the redwood dataset, we have a <span class="math inline">\(\chi^2\)</span> statistic of 202.52 which gives us a p-value &lt;&lt;&lt; 0.05 .Since we get a p-Value less than the significance level of 0.05, we reject the null hypothesis, (remember, that the data pattern is a realisation of CSR) and therefore, we conclude that the dataset isn’t randomnly spaced and is either clustered or dispersed.</p>
<p><br></p>
</div>
<div id="problems-with-quadrat-methods." class="section level3">
<h3><span class="header-section-number">3.1.4</span> Problems with quadrat methods.</h3>
<p><br></p>
<p>There is a massive big problem with quadrat methods. As we have seen, the process of using a quadrat method involves dividing our space in different equal area regions. This exact process of partitioning the space in different plays a very significant role in the final result, as we can decide to divide the space in only 4 parts or in a 1,000,000 parts…</p>
<p><br></p>
<div id="choosing-to-divide-the-region-in-big-subspaces." class="section level4">
<h4><span class="header-section-number">3.1.4.1</span> Choosing to divide the region in big subspaces.</h4>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">nsquares =<span class="st"> </span><span class="dv">2</span>

<span class="kw">plot</span>(<span class="dt">x =</span> redwood$x, <span class="dt">y =</span> redwood$y, <span class="dt">pch=</span><span class="dv">16</span>,<span class="dt">cex=</span><span class="fl">0.5</span>, <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>
     , <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="kw">min</span>(redwood$y)-<span class="fl">0.1</span>,<span class="kw">max</span>(redwood$y)+<span class="fl">0.1</span>)
     , <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="kw">min</span>(redwood$x)-<span class="fl">0.1</span>,<span class="kw">max</span>(redwood$x)+<span class="fl">0.1</span>))
<span class="kw">par</span>(<span class="dt">new=</span><span class="ot">TRUE</span>)
<span class="kw">plot</span>(<span class="kw">quadratcount</span>(redwood,<span class="dt">nx =</span> nsquares, <span class="dt">ny =</span> nsquares),<span class="dt">add=</span>T)</code></pre></div>
<p><img src="index_files/figure-html/unnamed-chunk-38-1.png" /><!-- --></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">quadrat.test</span>(<span class="kw">quadratcount</span>(redwood,<span class="dt">nx =</span> nsquares, <span class="dt">ny =</span> nsquares))</code></pre></div>
<pre><code>## 
##  Chi-squared test of CSR using quadrat counts
##  Pearson X2 statistic
## 
## data:  
## X2 = 6.5161, df = 3, p-value = 0.1781
## alternative hypothesis: two.sided
## 
## Quadrats: 2 by 2 grid of tiles</code></pre>
<ul>
<li><span class="math inline">\(\chi^2\)</span> &gt; 0.05 -&gt; in this case we CAN’T reject the NULL hypothesis and we conclude the data is a realisation of CSR!!!</li>
<li>So, we have gone from saying that the Redwood seedlings dataset was not completely spatially randomn by using a grid of 10x10, to saying it is by using a grid of 2x2!</li>
<li>This is not a very robust methodology as we depend on selecting an appropiate subcell size!</li>
</ul>
<p><br></p>
</div>
</div>
</div>
<div id="nearest-neighbour-method" class="section level2">
<h2><span class="header-section-number">3.2</span> Nearest Neighbour Method</h2>
<p><br></p>
<p>In view of these shortcomings, the quadrat method above has for the most part been replaced by other methods. The simplest of these is based on the observation that if one simply looks at distances between points and their nearest neighbors in <span class="math inline">\(R\)</span> , then this provides a natural test statistic that requires no artificial partitioning scheme.</p>
<p>First point to raise is how do we calculate distances between points. There are multiple methods but we will be using the <em>Eucliden distance</em>:</p>
<p><span class="math display">\[ d(x,y) = \sqrt[]{(x_1 - y_1)^2 + (x_2 - y_2)^2} \]</span></p>
<p>As the name indicates, the nearest neighbour indicates the point which is closest out of all the points in the dataset. You can imagine that, if the dataset has it’s points very far away from each other, the average nearest distance will be quite big compared to a dataset with it’s points close to each other. Again, we need an objective measure to understand the nearest neighbour method under CSR.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">include_graphics</span>(<span class="kw">paste0</span>(source_path,<span class="st">&quot;/images/3_nn_diagram.PNG&quot;</span>))</code></pre></div>
<p><img src="images/3_nn_diagram.PNG" width="688" /></p>
<p><br></p>
<div id="nn-method-under-csr" class="section level3">
<h3><span class="header-section-number">3.2.1</span> NN-method under CSR</h3>
<p>To make these ideas precise, we must determine the probability distribution of nn-distance under CSR, and compare the observed nn-distance with this distribution. There are an array of possible tests that can be carried out to calculate CSR for nn-method. The most common one is the <strong>Clark-Evans test</strong></p>
<p>The Clark-Evans test involves a normal approximation of the mean of the nn distance (<span class="math inline">\(D\)</span>). When we construct this normal approximation, we get the mean and variance of the distribution.</p>
<ul>
<li>Mean: <span class="math inline">\(E(D) = 1/(2\sqrt[]{\lambda})\)</span></li>
<li>Variance: <span class="math inline">\(var(D) = (4-\pi)/(4\lambda\pi)\)</span></li>
</ul>
<p>By quickly looking at both, we see that they depend on the point density <span class="math inline">\(\lambda\)</span>, which make total sense. For example, if we have a very dense point dataset, then the expected value and the variance of nn-distances decrease, in other words, the expected nn-distance is going to be smaller, and the variance of nn-distances across the dataset should be less.</p>
<p>By considering the Central Limit Theorem and other assumptions, we can say that the mean nn-distance for a studied sample must be approximately normally distributed under the CSR Hypothesis with mean and variance. This distribution provides a new test of the CSR Hypothesis, known as the Clark-Evans Test. All of this sounds a bit too technical to be fair, so build this test in a very simple way.</p>
<ul>
<li>First of all, we can construct a sample mean value for nn-distances <span class="math display">\[ d_m = \frac{1}{m} * \sum_{i=1}^m d_i \]</span></li>
<li>Using this, or the mean and variance from above, we can convert this into a <span class="math inline">\(Z\)</span> value</li>
</ul>
<p><span class="math display">\[ z_m = \frac{d_m- \mu}{\sigma}\]</span></p>
<ul>
<li>This <span class="math inline">\(Z\)</span> value is now ready to compare against a 2 tail or 1 tail test!</li>
</ul>
<p><br></p>
<div id="two-tail-tests" class="section level4">
<h4><span class="header-section-number">3.2.1.1</span> Two tail tests</h4>
<p><br></p>
<p>The standard test of CSR in most software is a two-tailed test in which both the possibility of “significantly small” values of <span class="math inline">\(d_m\)</span> (clustering) and “significantly large” values of <span class="math inline">\(d_m\)</span> (dispersion) are considered. If you do not remember very well what you learnt in your university statistics modules, let me give you a small summary.</p>
<p>A two-tailed test is a statistical test in which the critical area of a distribution is two-sided and tests whether a sample is greater than or less than a certain range of values. If you are using a significance level of 0.05, a two-tailed test allots half of your alpha to testing the statistical significance in one direction and half of your alpha to testing statistical significance in the other direction. This means that .025 is in each tail of the distribution of your test statistic.</p>
<p>We can use this test to decide if our dataset is spatially random (CSR) or not:</p>
<ul>
<li>Do not reject the CSR Hypothesis if <span class="math inline">\(|z_m| &lt;= z_{\alpha / 2}\)</span></li>
<li>Reject the CSR Hypothesis if <span class="math inline">\(|z_m| &gt; z_{\alpha / 2}\)</span>. Basically, we can say that the dataset is not randomnly space, but, we cannot determine with this test if it is clustered or dispersed.</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">include_graphics</span>(<span class="kw">paste0</span>(source_path,<span class="st">&quot;/images/5_two_tail_test.PNG&quot;</span>))</code></pre></div>
<p><img src="images/5_two_tail_test.PNG" width="536" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">include_graphics</span>(<span class="kw">paste0</span>(source_path,<span class="st">&quot;/images/5_1_one_tail_clustering.PNG&quot;</span>))</code></pre></div>
<p><img src="images/5_1_one_tail_clustering.PNG" width="418" /></p>
</div>
<div id="one-tail-test" class="section level4">
<h4><span class="header-section-number">3.2.1.2</span> One tail test</h4>
<p><br></p>
<p>In many cases, knowing if the dataset is clustered or dispersed is more relevant than simply knowing it is not randomnly spaced. This is when we use a 1 tail test.</p>
<ul>
<li>Clustering versus CSR Test: Significant if <span class="math inline">\(|z_m| &lt; -z_{\alpha}\)</span></li>
<li>Dispersion versus CSR Test: Significant if <span class="math inline">\(|z_m| &gt; z_{\alpha}\)</span></li>
</ul>
<p><img src="images/6_one_tail_clustering.PNG" width="558" /><img src="images/7_one_tail_dispersion.PNG" width="542" /><img src="images/6_1_one_tail_dispersion.PNG" width="366" /></p>
<p><br></p>
</div>
</div>
<div id="example" class="section level3">
<h3><span class="header-section-number">3.2.2</span> Example</h3>
<p><br></p>
<p>Let’s plot again the Redwood seedling example. Before starting any testing using the nn-method, remember that the quadrat method changed the result depending on the area of the subcells we divided our window into, and that was the reason we wanted to investigate new techniques.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">xlabs =<span class="st"> </span><span class="kw">list</span>(<span class="dt">title =</span> <span class="st">&quot;&quot;</span>, <span class="dt">linecolor =</span> <span class="kw">toRGB</span>(<span class="st">&quot;lightgrey&quot;</span>), <span class="dt">zerolinecolor =</span> <span class="kw">toRGB</span>(<span class="st">&quot;white&quot;</span>))
ylabs =<span class="st"> </span><span class="kw">list</span>(<span class="dt">title =</span> <span class="st">&quot;&quot;</span>, <span class="dt">linecolor =</span> <span class="kw">toRGB</span>(<span class="st">&quot;lightgrey&quot;</span>), <span class="dt">zerolinecolor =</span> <span class="kw">toRGB</span>(<span class="st">&quot;white&quot;</span>))

<span class="co"># ---------------------------------------------------------------------------------------------------------</span>
<span class="co"># Redwood seedlings dataset</span>
<span class="co"># ---------------------------------------------------------------------------------------------------------</span>
redwood_df =<span class="st"> </span><span class="kw">data.frame</span>(<span class="kw">matrix</span>(<span class="kw">vector</span>(), redwood$n, <span class="dv">2</span>
                               , <span class="dt">dimnames =</span> <span class="kw">list</span>(<span class="kw">c</span>(), <span class="kw">c</span>(<span class="st">&quot;x&quot;</span>,<span class="st">&quot;y&quot;</span>))), <span class="dt">stringsAsFactors =</span> F)
redwood_df$x =<span class="st"> </span>redwood$x
redwood_df$y =<span class="st"> </span>redwood$y

a1 =<span class="st"> </span><span class="kw">list</span>(<span class="dt">text =</span> <span class="st">&quot;Fig 1.1. Redwood seedlings&quot;</span>, <span class="dt">showarrow =</span> <span class="ot">FALSE</span>, <span class="dt">xref =</span> <span class="st">&quot;paper&quot;</span>, <span class="dt">yref =</span> <span class="st">&quot;paper&quot;</span>, <span class="dt">yanchor =</span> <span class="st">&quot;bottom&quot;</span>,
  <span class="dt">xanchor =</span> <span class="st">&quot;center&quot;</span>, <span class="dt">align =</span> <span class="st">&quot;center&quot;</span>, <span class="dt">x =</span> <span class="fl">0.5</span>, <span class="dt">y =</span> <span class="dv">1</span>)
p1 =<span class="st"> </span><span class="kw">plot_ly</span>(redwood_df, <span class="dt">x =</span> ~x, <span class="dt">y =</span> ~y) %&gt;%<span class="st"> </span><span class="kw">layout</span>(<span class="dt">annotations =</span> a1, <span class="dt">xaxis =</span> xlabs, <span class="dt">yaxis =</span> ylabs)
p1</code></pre></div>
<div id="2a7c2cda296a" style="width:864px;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="2a7c2cda296a">{"x":{"visdat":{"2a7c596f53c8":["function () ","plotlyVisDat"]},"cur_data":"2a7c596f53c8","attrs":{"2a7c596f53c8":{"x":{},"y":{},"alpha":1,"sizes":[10,100]}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"annotations":[{"text":"Fig 1.1. Redwood seedlings","showarrow":false,"xref":"paper","yref":"paper","yanchor":"bottom","xanchor":"center","align":"center","x":0.5,"y":1}],"xaxis":{"domain":[0,1],"title":"","linecolor":"rgba(211,211,211,1)","zerolinecolor":"rgba(255,255,255,1)"},"yaxis":{"domain":[0,1],"title":"","linecolor":"rgba(211,211,211,1)","zerolinecolor":"rgba(255,255,255,1)"},"hovermode":"closest","showlegend":false},"source":"A","config":{"modeBarButtonsToAdd":[{"name":"Collaborate","icon":{"width":1000,"ascent":500,"descent":-50,"path":"M487 375c7-10 9-23 5-36l-79-259c-3-12-11-23-22-31-11-8-22-12-35-12l-263 0c-15 0-29 5-43 15-13 10-23 23-28 37-5 13-5 25-1 37 0 0 0 3 1 7 1 5 1 8 1 11 0 2 0 4-1 6 0 3-1 5-1 6 1 2 2 4 3 6 1 2 2 4 4 6 2 3 4 5 5 7 5 7 9 16 13 26 4 10 7 19 9 26 0 2 0 5 0 9-1 4-1 6 0 8 0 2 2 5 4 8 3 3 5 5 5 7 4 6 8 15 12 26 4 11 7 19 7 26 1 1 0 4 0 9-1 4-1 7 0 8 1 2 3 5 6 8 4 4 6 6 6 7 4 5 8 13 13 24 4 11 7 20 7 28 1 1 0 4 0 7-1 3-1 6-1 7 0 2 1 4 3 6 1 1 3 4 5 6 2 3 3 5 5 6 1 2 3 5 4 9 2 3 3 7 5 10 1 3 2 6 4 10 2 4 4 7 6 9 2 3 4 5 7 7 3 2 7 3 11 3 3 0 8 0 13-1l0-1c7 2 12 2 14 2l218 0c14 0 25-5 32-16 8-10 10-23 6-37l-79-259c-7-22-13-37-20-43-7-7-19-10-37-10l-248 0c-5 0-9-2-11-5-2-3-2-7 0-12 4-13 18-20 41-20l264 0c5 0 10 2 16 5 5 3 8 6 10 11l85 282c2 5 2 10 2 17 7-3 13-7 17-13z m-304 0c-1-3-1-5 0-7 1-1 3-2 6-2l174 0c2 0 4 1 7 2 2 2 4 4 5 7l6 18c0 3 0 5-1 7-1 1-3 2-6 2l-173 0c-3 0-5-1-8-2-2-2-4-4-4-7z m-24-73c-1-3-1-5 0-7 2-2 3-2 6-2l174 0c2 0 5 0 7 2 3 2 4 4 5 7l6 18c1 2 0 5-1 6-1 2-3 3-5 3l-174 0c-3 0-5-1-7-3-3-1-4-4-5-6z"},"click":"function(gd) { \n        // is this being viewed in RStudio?\n        if (location.search == '?viewer_pane=1') {\n          alert('To learn about plotly for collaboration, visit:\\n https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html');\n        } else {\n          window.open('https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html', '_blank');\n        }\n      }"}],"cloud":false},"data":[{"x":[0.36,0.44,0.48,0.48,0.5,0.76,0.78,0.78,0.84,0.86,0.9,0.9,0.9,0.9,0.18,0.18,0.2,0.6,0.62,0.64,0.64,0.68,0.68,0.7,0.72,0.1,0.12,0.12,0.14,0.18,0.18,0.2,0.2,0.22,0.24,0.52,0.5,0.58,0.94,0.96,0.22,0.22,0.26,0.44,0.48,0.5,0.9,0.9,0.48,0.52,0.56,0.999,0.44,0.48,0.34,0.38,0.26,0.4,0.28,0.74,0.86,0.96],"y":[-0.08,-0.1,-0.08,-0.14,-0.1,-0.14,-0.12,-0.16,-0.08,-0.18,-0.08,-0.1,-0.16,-0.2,-0.4,-0.38,-0.42,-0.34,-0.34,-0.36,-0.28,-0.32,-0.24,-0.28,-0.26,-0.58,-0.58,-0.62,-0.58,-0.56,-0.54,-0.52,-0.5,-0.46,-0.48,-0.58,-0.6,-0.58,-0.52,-0.54,-0.8,-0.84,-0.7,-0.76,-0.78,-0.76,-0.76,-0.78,-0.68,-0.66,-0.64,-0.84,-0.82,-0.82,-0.84,-0.84,-0.86,-0.86,-0.86,-0.9,-0.9,-0.96],"type":"scatter","mode":"markers","marker":{"fillcolor":"rgba(31,119,180,1)","color":"rgba(31,119,180,1)","line":{"color":"transparent"}},"xaxis":"x","yaxis":"y","frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1}},"base_url":"https://plot.ly"},"evals":["config.modeBarButtonsToAdd.0.click"],"jsHooks":{"render":[{"code":"function(el, x) { var ctConfig = crosstalk.var('plotlyCrosstalkOpts').set({\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1}}); }","data":null}]}}</script>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Number of points</span>
<span class="kw">print</span>(<span class="kw">paste</span>(<span class="st">&#39;Number of points: &#39;</span>, redwood$n))</code></pre></div>
<pre><code>## [1] &quot;Number of points:  62&quot;</code></pre>
<p>Let’s now calculate the standard <span class="math inline">\(Z\)</span> values and the correspoding <span class="math inline">\(p-values\)</span> associated with them:</p>
<ul>
<li>Point density: $ = n/a(R) = 62/44108 = 0.00141$</li>
<li>CSR Mean nn-distance: $ = 1/2 = 1/2 = 13.336 $</li>
<li>Sample Mean nn-distance: $ d_m = 9.037 $</li>
<li>CSR Standard dev of nn-distance: $ =  =  = 0.8853$</li>
</ul>
<p>At this point, we already notice that the redwood seedling dataset has an average nn-distance smaller than the one for a CSR dataset under a certain point density (9.037 &lt; 13.336). This already suggests that the redwood seedling dataset has individual trees that are much closer to their nearest individual nn-tree than if the spacing was random. Let’s check this statistically.</p>
<ul>
<li><span class="math inline">\(Z\)</span> value: <span class="math inline">\(z_m = (d_m - \mu)/(sigma) = (9.037 - 13.336)/(0.8853) = -4.855\)</span></li>
<li><p>Comparing with the table above for a 1 tail test <span class="math inline">\(Z_sample = -4.855 &lt; -2.33 = -z_{\alpha=0.01}\)</span>, which means that there is significant clustering.</p></li>
<li><p><span class="math inline">\(P-value\)</span>: <span class="math inline">\(P(Z\leq z_m) = \Phi (z_m) = \Phi (-4.855) = 0.0000006\)</span>. This is telling us that, the chance of obtaining a mean nn-distance as low as 13.336 is less than one in a million, which is very strong evidence in favor of clustering versus CSR.</p></li>
</ul>
<p><br></p>
<div id="plots-for-nn-methods---g-function" class="section level4">
<h4><span class="header-section-number">3.2.2.1</span> Plots for NN-methods - G function</h4>
<p><br></p>
<p>We can find and plot nearest neighbour distances, finding them with nndist - plotting the empirical cumulative distribution function (ECDF) of the nearest neighbour distances is interesting.</p>
<p>At the same time we can compare this to the <span class="math inline">\(G\)</span> measure, which turns out to be just the ECDF of the nearest neighbour distances, plotted by default with the expected CSR line. In other words, it measures the distribution of the distances from an arbitrary event to its nearest event. Under CSR, the value of the G function is:</p>
<p><span class="math display">\[ G(d) = 1 - exp(-\lambda \pi d^2)\]</span></p>
<p>Instead of calculating the <span class="math inline">\(G\)</span> function for only a given distance, we can apply Monte-Carlo methods (putting it simple, calculating the G function multiple times for different random samples) which will provide the upper and lower envelopes (upper and lower accepted boundaries.):</p>
<ul>
<li>If the empirical <span class="math inline">\(G\)</span> function is within the boundaries, then it can be accepted that it complies with the <span class="math inline">\(G\)</span> function under CSR</li>
<li>If it is above the maximum boundary, this suggests a clustered pattern.</li>
<li>If it is below the minimum boundary, this suggests a regular pattern.</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">123</span>)

<span class="co"># Number points</span>
n =<span class="st"> </span>redwood$n

<span class="co"># Redwood window</span>
redwood_rr =<span class="st"> </span><span class="kw">ppp</span>(redwood$x, redwood$y, <span class="dt">window =</span> <span class="kw">ripras</span>(redwood))

<span class="co"># Creating multiple random point patterns with n points and within the redwood seedling window</span>
ex =<span class="st"> </span><span class="kw">expression</span>(<span class="kw">runifpoint</span>(n, <span class="dt">win =</span> <span class="kw">ripras</span>(redwood)))

<span class="co"># Calculate the upper and lower boundaries</span>
res =<span class="st"> </span><span class="kw">envelope</span>(redwood_rr, Gest, <span class="dt">nsim =</span> <span class="dv">99</span>, <span class="dt">simulate =</span> ex, <span class="dt">verbose =</span> <span class="ot">FALSE</span>, <span class="dt">saveall =</span> <span class="ot">TRUE</span>)

<span class="kw">plot</span>(res, <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="fl">0.2</span>))</code></pre></div>
<p><img src="index_files/figure-html/unnamed-chunk-45-1.png" /><!-- --></p>
<p><br></p>
<p>When reading the graph, values <span class="math inline">\(G_{obs}(r) &gt; G(r)\)</span> suggest that nearest neighbour distances in the point pattern are shorter than for a Poisson process, suggesting a clustered pattern.</p>
<p><br></p>
</div>
</div>
<div id="problems-with-nn-methods" class="section level3">
<h3><span class="header-section-number">3.2.3</span> Problems with NN-methods</h3>
<p><br></p>
<p>One major difficulty with using only the <span class="math inline">\(Z\)</span> values, compared to the <span class="math inline">\(G\)</span> function, is that we have used the entire point pattern, and have thus ignored the obviously dependencies between nn-distances. This procedure tends to overestimate the significance of clustering (or dispersion). Let’s use another dataset to understand the problems that nn-methods might have.</p>
<p><br></p>
<div id="bodmin-tors-example" class="section level4">
<h4><span class="header-section-number">3.2.3.1</span> Bodmin Tors example</h4>
<p><br></p>
<p>The Redwood Seedling example above is something of a “straw man” in that statistical analysis is hardly required to demonstrate the presence of such obvious clustering. Our second example provides a good case in point. It also serves to illustrate some additional limitations of the above analysis.</p>
<p>Here the point pattern consists of granite outcroppings (tors) in the Bodmin Moor, located at the very southern tip of England in Cornwall county. It seems to appear that there is some clustering of tors, but certainly not as strong as the redwood seedling example above.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(bodmin)

bodmin1 =<span class="st"> </span>bodmin

<span class="co"># A splancs point to spatstat ppp class conversion function</span>
spl2ppp &lt;-<span class="st"> </span>function(splancs.obj) {
    p1 &lt;-<span class="st"> </span>splancs.obj$poly
    end &lt;-<span class="st"> </span><span class="kw">length</span>(p1[,<span class="dv">1</span>])
    <span class="co"># The following loop removes any duplicated top and bottom vertices</span>
    while (<span class="kw">sum</span>(p1[end,] ==<span class="st"> </span>p1[<span class="dv">1</span>,]) ==<span class="st"> </span><span class="dv">2</span>) {
        p1 &lt;-<span class="st"> </span>p1[-<span class="dv">1</span>,]
        end &lt;-<span class="st"> </span><span class="kw">length</span>(p1[,<span class="dv">1</span>])
    }
    <span class="co"># The following loop removes any duplicated adjacent vertices</span>
    end &lt;-<span class="st"> </span><span class="kw">length</span>(p1[,<span class="dv">1</span>])
    dupeVec &lt;-<span class="st"> </span><span class="kw">vector</span>()
    for(i in <span class="dv">1</span>:(end<span class="dv">-1</span>)) {
        if(<span class="kw">sum</span>(p1[i,] ==<span class="st"> </span>p1[i<span class="dv">+1</span>,]) ==<span class="st"> </span><span class="dv">2</span>) {
            dupeVec &lt;-<span class="st"> </span><span class="kw">c</span>(dupeVec, i)
        }
    }
    p1 &lt;-<span class="st"> </span>p1[-dupeVec,]
    p2 &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">x=</span>p1[,<span class="dv">1</span>], <span class="dt">y=</span>p1[,<span class="dv">2</span>])  <span class="co"># change to class list</span>
    p3 &lt;-<span class="st"> </span><span class="kw">owin</span>(<span class="dt">poly=</span>p2)
    pp1 &lt;-<span class="st"> </span><span class="kw">cbind</span>(splancs.obj$x,splancs.obj$y)
    ppp.obj &lt;-<span class="st"> </span><span class="kw">as.ppp</span>(pp1, <span class="dt">W=</span>p3)
    <span class="kw">return</span>(ppp.obj)
}

bod1 =<span class="st"> </span><span class="kw">spl2ppp</span>(bodmin1)
bod =<span class="st"> </span><span class="kw">spl2ppp</span>(bodmin)

<span class="kw">plot</span>(bod1, <span class="dt">cols=</span><span class="st">&#39;brown&#39;</span>, <span class="dt">main=</span><span class="st">&#39;Bodmin Tors&#39;</span>) </code></pre></div>
<p><img src="index_files/figure-html/unnamed-chunk-46-1.png" /><!-- --></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Number of point</span>
<span class="kw">print</span>(<span class="kw">paste</span>(<span class="st">&#39;Number of points: &#39;</span>, bod$n))</code></pre></div>
<pre><code>## [1] &quot;Number of points:  35&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Area of shape</span>
<span class="kw">print</span>(<span class="kw">paste</span>(<span class="st">&#39;Area: &#39;</span>, <span class="kw">area</span>(bod)))</code></pre></div>
<pre><code>## [1] &quot;Area:  206.62&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Mean nn-distance</span>
nns =<span class="st"> </span><span class="kw">nndist</span>(bod)

nn_calculations =<span class="st"> </span>function(n, area, mean_sample_nn_dist){
  
  lambda =<span class="st"> </span>n/area
  mean_nn_dist =<span class="st"> </span><span class="dv">1</span>/(<span class="dv">2</span>*<span class="kw">sqrt</span>(lambda))
  sigma =<span class="st"> </span><span class="kw">sqrt</span>(<span class="dv">4</span><span class="fl">-3.1415</span>)/(<span class="kw">sqrt</span>(<span class="dv">4</span>*n*<span class="fl">3.1415</span>*lambda))
  
  z_value =<span class="st"> </span>(mean_sample_nn_dist -<span class="st"> </span>mean_nn_dist)/(sigma)  
  
  <span class="kw">print</span>(<span class="kw">paste</span>(<span class="st">&quot;Point density = &quot;</span>, <span class="kw">round</span>(lambda,<span class="dv">3</span>)))
  <span class="kw">print</span>(<span class="kw">paste</span>(<span class="st">&quot;CSR Mean nn-distance = &quot;</span>, <span class="kw">round</span>(mean_nn_dist,<span class="dv">3</span>)))
  <span class="kw">print</span>(<span class="kw">paste</span>(<span class="st">&quot;CSR Std dev of nn-distance = &quot;</span>, <span class="kw">round</span>(sigma,<span class="dv">3</span>)))
  <span class="kw">print</span>(<span class="kw">paste</span>(<span class="st">&quot;Z value = &quot;</span>, <span class="kw">round</span>(z_value,<span class="dv">3</span>)))
}

<span class="kw">nn_calculations</span>(bod$n, <span class="kw">area</span>(bod), <span class="kw">mean</span>(nns))</code></pre></div>
<pre><code>## [1] &quot;Point density =  0.169&quot;
## [1] &quot;CSR Mean nn-distance =  1.215&quot;
## [1] &quot;CSR Std dev of nn-distance =  0.107&quot;
## [1] &quot;Z value =  -1.033&quot;</code></pre>
<p><br></p>
<p>** CALCULATION WITH FULL SAMPLE **</p>
<ul>
<li>Point density: $ = n/a(R) = 35/206.62 = 0.169 $</li>
<li>CSR Mean nn-distance: $ = 1/2 = 1/2 = 1.216 $</li>
<li>Sample Mean nn-distance: $ d_m = 1.1039 $</li>
<li>CSR Standard dev of nn-distance: $ =  =  = 0.107$</li>
</ul>
<p>At this point, we notice that the Bodmin tor dataset has an average nn-distance smaller than the one for a CSR dataset under a certain point density (9.037 &lt; 13.336), BUT, in this case the differences are negligible compared to the differences occured for the Redwood seedling dataset!</p>
<ul>
<li><span class="math inline">\(Z\)</span> value: <span class="math inline">\(z_m = (d_m - \mu)/(sigma) = (1.1039 - 1.215)/(0.107) = -1.033\)</span></li>
<li>Comparing with the table above for a 1 tail test <span class="math inline">\(Z_sample = -1.033 &gt; -2.33 = -z_{\alpha=0.01}\)</span>, which means that there we cannot reject the hypothesis of CSR. It does shock a little bit, because intuitively there seems to be some clustering of points at the left side of the studied area.</li>
</ul>
<p>Let’s check what the G-function gives us.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">123</span>)

<span class="co"># Number points</span>
n =<span class="st"> </span>bod$n

<span class="co"># Creating multiple random point patterns with n points and within the bodmin tor window</span>
ex =<span class="st"> </span><span class="kw">expression</span>(<span class="kw">runifpoint</span>(n, <span class="dt">win =</span> <span class="kw">ripras</span>(bod)))

<span class="co"># Calculate the upper and lower boundaries</span>
res =<span class="st"> </span><span class="kw">envelope</span>(bod, Gest, <span class="dt">nsim =</span> <span class="dv">99</span>, <span class="dt">simulate =</span> ex, <span class="dt">verbose =</span> <span class="ot">FALSE</span>, <span class="dt">saveall =</span> <span class="ot">TRUE</span>)

<span class="kw">plot</span>(res, <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">4</span>))</code></pre></div>
<p><img src="index_files/figure-html/unnamed-chunk-48-1.png" /><!-- --></p>
<p><br></p>
<p>By using the <span class="math inline">\(G\)</span> function, we would also say that the Bodming Tor dataset complies with CSR, but as we said before, we are not really convinced about this result as the plotting of the data suggest some possible clustering. This could be due to 2 key theoretical difficulties here that have yet to be addressed: * The first is that for point pattern samples as small as the Bodmin Tors example, the assumption of asymptotic normality may be questionable. * The second is that nn-distances for points near the boundary of region R are not distributed the same as those away from the boundary.</p>
<p><br></p>
</div>
</div>
</div>
<div id="k-functions" class="section level2">
<h2><span class="header-section-number">3.3</span> K functions</h2>
<p><br></p>
<p>We therefore need to add another tweak to the previous 2 methods. Remember that, due to the problems related with dependancies of the area of subcells in the quadrat methods, we tried to get away from that methodology and work with distances between points.</p>
<p>In the Bodmin Tors example above, notice that the clustering structure is actually quite different from that of the Redwood Seedling example. Rather than small isolated clumps, there appear to be two large groups of points in the northwest and southwest, separated by a large empty region. Moreover, the points within each group are actually quite evenly spaced (locally dispersed). These observations suggest that the pattern of tors exhibits different structures at different scales. Hence the objective of the present section is to introduce a method of point pattern nalysis that takes such scale effects into account, and in fact allows “scale” to become a fundamental variable in the analysis.</p>
<p>To capture a range of scales in a more systematic way, we now consider what amounts to an extension of the quadrat (or cell-count) method. If in quadrat methods, dependencies on scale of individual cells was a weakness, in K functions we are going to turn this dependency into a virtue. To do this, rather than fixing the location and the scale of the subcells (quadrat method), we are going to allow cells to vary in size and at the same time, consider randomnly sampled cells. Remember that, in the first method (fixing location and scale of subcells), we could very easily calculate the expected number of events at any given location as they followed a theoretical point density of <span class="math inline">\(number points total/area\)</span>. In K functions, we need to consider varying distances from any given point, and this measure doesn’t make sense if we don’t add the point density mentioned. For example, if the point density was very high (very heavy intense rain), then we would expect to find many points within distance <span class="math inline">\(h\)</span> of the initial point. So, in order to eliminate the effect of point densities, we can simply divide by the point density.</p>
<p><br></p>
<div id="building-the-formula" class="section level3">
<h3><span class="header-section-number">3.3.1</span> Building the formula</h3>
<p><br></p>
<div id="simple-k-function" class="section level4">
<h4><span class="header-section-number">3.3.1.1</span> Simple K-function</h4>
<p><br></p>
<p>This takes us to introduce the first version (the simplest) of the K-function:</p>
<p><span class="math display">\[ K(h) = \frac{E(number\:of\:additional\:events\:within\:distance\:h\:of\:an\:arbitrary\:event)}{\lambda}\]</span></p>
<p>As always, we will need to understand what is the measure of <span class="math inline">\(K(h)\)</span> under a CSR hypothesis. To illustrate why we need this, let’s consider the following 2 images:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">include_graphics</span>(<span class="kw">paste0</span>(source_path,<span class="st">&quot;/images/8_k_function.PNG&quot;</span>))</code></pre></div>
<p><img src="images/8_k_function.PNG" width="608" /></p>
<p><br></p>
<p>The left image shows a distribution of wolf packs. Clearly, this point pattern seems dispersed (as one would expect from wolf packs, as they each have a hunting terriorty). However, within each pack, we have individual wolfs, who are clustered together within each pack. Understanding this, we turn back to our <span class="math inline">\(K(h)\)</span>. If one were to define <span class="math inline">\(K(h)\)</span> with respect to small distances, <span class="math inline">\(h\)</span> , around each wolf in the right image, then given the close proximity to other wolves in the same pack, these values would surely be too high to be consistent with CSR for the given density of wolves in this area, which would obvisouly imply clustering. Similarly, if one were to define <span class="math inline">\(K(h)\)</span> with respect to much larger distances, h , around each wolf in the left figure then given the wide spacing between wolf packs (and the relative uniformity of wolf-pack sizes), these values would surely be too low to be consistent with CSR for the given density of wolves. Hence if one can identify appropriate bench-mark values for <span class="math inline">\(K(h)\)</span> under CSR, then these K-functions can be used to test for clustering and dispersion at various scales of analysis.</p>
<p><br></p>
</div>
<div id="counting-number-of-points-within-distance-h" class="section level4">
<h4><span class="header-section-number">3.3.1.2</span> Counting number of points within distance h</h4>
<p><br></p>
<p>From the <span class="math inline">\(K(h)\)</span> above, we can start adding elements to it. For example, we want to to translate the <em>number of events within distance h</em> into a mathematical function. It’s as easy as defining an indicator function:</p>
$$ I_{h}(d_{ij}) = I_{h}[d(s_{i},s_{j})] = {
<span class="math display">\[\begin{array}{ll}
        1  &amp; \mbox{if } d_{ij} \geq h \\
        0 &amp; \mbox{if } d_{ij} &lt; h
    \end{array}\]</span>
<p>.</p>
<p>$$</p>
<p>Using the indicator function in the K-function:</p>
<p><span class="math display">\[ K(h) = \frac{E[\sum_{j \neq i} I_h(d_{ij})]}{\lambda} \]</span></p>
<p>And, instead of applying the K function to all points, if we were to create the sample version of K:</p>
<p><span class="math display">\[ K(h) = \frac{E[\sum_{j \neq i} I_h(d_{ij})]}{\lambda * n} \]</span></p>
<p><br></p>
</div>
<div id="taking-into-account-edge-effects" class="section level4">
<h4><span class="header-section-number">3.3.1.3</span> Taking into account edge effects</h4>
<p><br></p>
<p>However, the previous formula can never hold exactly in bounded regions R, due to edge effects. Let’s understand the problem of edge effects. In the figure below, we have a point <span class="math inline">\(s_i\)</span> next to the edge of study region <span class="math inline">\(R\)</span>. If we count the points that are within distance <span class="math inline">\(h\)</span>, we would count only 3 points. However, the theoretical expectectation of points, would try to calculate this using the entire circle. Therefore, if we dont take into account edge effects, the expected value of point counts are reduced.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">include_graphics</span>(<span class="kw">paste0</span>(source_path,<span class="st">&quot;/images/9_k_function.PNG&quot;</span>))</code></pre></div>
<p><img src="images/9_k_function.PNG" width="267" /></p>
<p><br></p>
<p>In order to correct the effect, we weight each point, <span class="math inline">\(s_j\)</span>, in the count <span class="math inline">\(I_h(d_{ij})\)</span> in a manner that inflates counts for points near the boundary. So basically, for points that are not next to the boundary, the weighting will = 1 because we can fill the whole circle. On another hand, if the point is next to the boundary, the circle gets splits by the <span class="math inline">\(R\)</span> boundary, so the weight is less than one. This will make the weighting for that particular point higher than 1.</p>
<p><span class="math display">\[ K(h) = \frac{E[\sum_{j \neq i} I_h(d_{ij})]}{\lambda * n} * \frac{1}{w_{ij}}\]</span></p>
<p><br></p>
</div>
</div>
<div id="k-and-l-function-under-csr" class="section level3">
<h3><span class="header-section-number">3.3.2</span> K and L function under CSR</h3>
<p><br></p>
<p>To apply K-functions in testing the CSR Hypothesis, it is convenient to begin by ignoring edge effects, and considering the nature of K-functions under this hypothesis for points, that are not influenced by edge effects. It turns out that, for these points, the CSR hypothesis is as simple as:</p>
<p><span class="math display">\[ K(h) = \frac{\lambda \pi h^2}{\lambda} = \pi h^2 \]</span></p>
<p>So:</p>
<ul>
<li>If <span class="math inline">\(K(h) &gt; \pi h^2\)</span> -&gt; clustering, because the mean point count is higher than CSR</li>
<li>If <span class="math inline">\(K(h) &lt; \pi h^2\)</span> -&gt; dispersion, because the mean point count is lower than CSR</li>
</ul>
<p>If you want to transform this into a starndardised function, its convenient to standardise area values. Therefore:</p>
<p><span class="math display">\[ L(h) = \sqrt{\frac{K(h)}{\pi}} - h = \sqrt{\frac{\pi h^2}{\pi}} - h = h - h = 0\]</span> So:</p>
<ul>
<li>If $L(h) &gt; 0 -&gt; clustering.</li>
<li>If $L(h) &lt; 0 -&gt; dispersion</li>
</ul>
<p><br></p>
</div>
<div id="testing-the-bodmin-tor-example" class="section level3">
<h3><span class="header-section-number">3.3.3</span> Testing the Bodmin Tor example</h3>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">123</span>)

bodmin_K =<span class="st"> </span><span class="kw">Kest</span>(bod, <span class="dt">correction=</span><span class="st">&quot;Ripley&quot;</span>)
bodmin_K_envelopes =<span class="st"> </span><span class="kw">envelope</span>(bod, Kest)</code></pre></div>
<pre><code>## Generating 99 simulations of CSR  ...
## 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75,
## 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98,  99.
## 
## Done.</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(bodmin_K_envelopes, <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">4</span>))</code></pre></div>
<p><img src="index_files/figure-html/unnamed-chunk-51-1.png" /><!-- --></p>
<p><br></p>
<p><strong>How to interpret the graphs?</strong> * K function graph ** The red line represents the theoretical K function under CSR and the envelopes are represented by the upper and lower borders of the grey area (these are calculated by randomnly creating sample points and calculating R) ** The black line represents the observed K function for the Bodmin Tor dataset. ** We can interpret that, for the Bodmin Tor example, for distances below 1, we more or less follow a CSR pattern. Between 1 and 2, we comply with the expected uncertainty under CSR. But after 2, the pattern shows clustering. ** This conclusion seems quite logical because we did expect at least some clustering due to the distribution of the points. If you look at the following graph, when we add circles of expanding radius, more points appear (radius = 2 starts from red line).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">123</span>)

<span class="kw">plot</span>(bod$x,bod$y)
<span class="kw">symbols</span>(<span class="dt">x=</span><span class="kw">c</span>(bod$x[<span class="dv">20</span>]), <span class="dt">y=</span><span class="kw">c</span>(bod$y[<span class="dv">20</span>]), <span class="dt">circles=</span><span class="kw">rep</span>(<span class="fl">0.5</span>), <span class="dt">add=</span>T, <span class="dt">inches=</span>F)
<span class="kw">symbols</span>(<span class="dt">x=</span><span class="kw">c</span>(bod$x[<span class="dv">20</span>]), <span class="dt">y=</span><span class="kw">c</span>(bod$y[<span class="dv">20</span>]), <span class="dt">circles=</span><span class="kw">rep</span>(<span class="dv">1</span>), <span class="dt">add=</span>T, <span class="dt">inches=</span>F)
<span class="kw">symbols</span>(<span class="dt">x=</span><span class="kw">c</span>(bod$x[<span class="dv">20</span>]), <span class="dt">y=</span><span class="kw">c</span>(bod$y[<span class="dv">20</span>]), <span class="dt">circles=</span><span class="kw">rep</span>(<span class="fl">1.5</span>), <span class="dt">add=</span>T, <span class="dt">inches=</span>F)
<span class="kw">symbols</span>(<span class="dt">x=</span><span class="kw">c</span>(bod$x[<span class="dv">20</span>]), <span class="dt">y=</span><span class="kw">c</span>(bod$y[<span class="dv">20</span>]), <span class="dt">circles=</span><span class="kw">rep</span>(<span class="dv">2</span>), <span class="dt">add=</span>T, <span class="dt">inches=</span>F, <span class="dt">fg =</span> <span class="st">&quot;red&quot;</span>)
<span class="kw">symbols</span>(<span class="dt">x=</span><span class="kw">c</span>(bod$x[<span class="dv">20</span>]), <span class="dt">y=</span><span class="kw">c</span>(bod$y[<span class="dv">20</span>]), <span class="dt">circles=</span><span class="kw">rep</span>(<span class="dv">3</span>), <span class="dt">add=</span>T, <span class="dt">inches=</span>F, <span class="dt">fg =</span> <span class="st">&quot;red&quot;</span>)</code></pre></div>
<p><img src="index_files/figure-html/unnamed-chunk-52-1.png" /><!-- --></p>
</div>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
